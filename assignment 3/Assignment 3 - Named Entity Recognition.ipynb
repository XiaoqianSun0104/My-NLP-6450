{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 - Named Entity Recognition\n",
    "\n",
    "In this assignment, we are going to build a Named Entity Recognition model. With this model, we will also tag new data.\n",
    "\n",
    "More on Named Entity Recognition:\n",
    "\n",
    "https://blog.paralleldots.com/data-science/named-entity-recognition-milestone-models-papers-and-technologies/\n",
    "\n",
    "https://blog.paralleldots.com/product/applications-named-entity-recognition-api/\n",
    "\n",
    "### Steps:\n",
    "\n",
    "**1. Import the data**\n",
    "\n",
    "**2. Build the model**\n",
    "\n",
    "**3. Pick a dataset to run the model on**\n",
    "\n",
    "**4. Build a function to load new data and print the tags**\n",
    "\n",
    "Your web application will load small sections of text (such as tweets or headlines) and from that, you will tag the text based on the presence of named entities.\n",
    "\n",
    "*What you will be graded on:*\n",
    "\n",
    "1. Ability to build a model on word and tag data\n",
    "\n",
    "2. Ability to use the model to predict on new data and display that prediction\n",
    "\n",
    "*The model will be based on:*\n",
    "1. Embeddings from words\n",
    "2. Embeddings from tag inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing the data\n",
    "\n",
    "Below is some code to get you started. As in the part of speech tagging example, you will have to write code to:\n",
    "\n",
    "0. Split your data into a train/test set (Do a 80/20 or 90/10 split since we'll be later applying this model to an entirely separate set of data)\n",
    "1. Find the set of all words\n",
    "2. Find the set of all tags\n",
    "3. **Create a function called ent_tagger** that will turn a sentence into this output for model building :\n",
    "``` [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have',  'O'), ('marched',  'O'), ('through',  'O'), ('London', 'B-geo'), ('to',  'O'), ('protest',  'O'), ('the',  'O'), ('war',  'O'), ('in',  'O'), ('Iraq',  'B-geo'), ('and', 'O'), ('demand',  'O'), ('the',  'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops',  'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n",
    "```\n",
    "4. Make a dictionary of words to index and entity tag to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the all words list(no repeat)\n",
    "words_list = data.Word.values.tolist()\n",
    "vocabulary = set(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the all tags list(no repeat)\n",
    "tags_list = data.Tag.values.tolist()\n",
    "tags_list_set = set(tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ent_tagger function will be created at the last question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary of words to index\n",
    "word2int = {}\n",
    "for i,word in enumerate(vocabulary):\n",
    "    word2int[word] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictionary of entity tag to index to index\n",
    "tag2int = {}\n",
    "for i, tag in enumerate(tags_list_set):\n",
    "    tag2int[tag] = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Formatting the data\n",
    "Data will need to be\n",
    "\n",
    "1. Indexed\n",
    "2. Limited by vocabulary (ie replace tokens with UNKNOWN if they are too rare, come up with a reasonable limit based on your survey of the data and also model performance)\n",
    "3. Padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      "LEXICON SAMPLE (35179 total items):\n",
      "{'Thousands': 2, 'of': 3, 'demonstrators': 4, 'have': 5, 'marched': 6, 'through': 7, 'London': 8, 'to': 9, 'protest': 10, 'the': 11, 'war': 12, 'in': 13, 'Iraq': 14, 'and': 15, 'demand': 16, 'withdrawal': 17, 'British': 18, 'troops': 19, 'from': 20, 'that': 21}\n",
      "TAGS:\n",
      "LEXICON SAMPLE (18 total items):\n",
      "{'O': 2, 'B-geo': 3, 'B-gpe': 4, 'B-per': 5, 'I-geo': 6, 'B-org': 7, 'I-org': 8, 'B-tim': 9, 'B-art': 10, 'I-art': 11, 'I-per': 12, 'I-gpe': 13, 'I-tim': 14, 'B-nat': 15, 'B-eve': 16, 'I-eve': 17, 'I-nat': 18, '<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "#create vocabulary and set words that appear less than 2 to unknown\n",
    "import pickle\n",
    "\n",
    "def make_lexicon(words_list, min_freq=1):\n",
    "    word_counts = {}\n",
    "    for word in words_list:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "\n",
    "    lexicon = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    lexicon = {word:idx + 2 for idx,word in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 \n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(\"LEXICON SAMPLE ({} total items):\".format(len(lexicon)))\n",
    "    print(dict(list(lexicon.items())[:20]))\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "print(\"WORDS:\")\n",
    "words_lexicon = make_lexicon(words_list)\n",
    "\n",
    "print(\"TAGS:\")\n",
    "tags_lexicon = make_lexicon(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'O', 3: 'B-geo', 4: 'B-gpe', 5: 'B-per', 6: 'I-geo', 7: 'B-org', 8: 'I-org', 9: 'B-tim', 10: 'B-art', 11: 'I-art', 12: 'I-per', 13: 'I-gpe', 14: 'I-tim', 15: 'B-nat', 16: 'B-eve', 17: 'I-eve', 18: 'I-nat', 1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "#a dictionary where the string representation of a lexicon item can be retrieved from its numerical index\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(\"LEXICON LOOKUP SAMPLE:\")\n",
    "    print(dict(list(lexicon_lookup.items())[:20]))\n",
    "    return lexicon_lookup\n",
    "\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word_Idxs</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Tag_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>3</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>5</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>6</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>7</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>8</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>9</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>10</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>11</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  Word_Idxs    Tag  Tag_Idxs\n",
       "0  Sentence: 1      Thousands          2      O         2\n",
       "1  Sentence: 1             of          3      O         2\n",
       "2  Sentence: 1  demonstrators          4      O         2\n",
       "3  Sentence: 1           have          5      O         2\n",
       "4  Sentence: 1        marched          6      O         2\n",
       "5  Sentence: 1        through          7      O         2\n",
       "6  Sentence: 1         London          8  B-geo         3\n",
       "7  Sentence: 1             to          9      O         2\n",
       "8  Sentence: 1        protest         10      O         2\n",
       "9  Sentence: 1            the         11      O         2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renew the dataframe\n",
    "def tokens_to_idxs(words_list, lexicon):\n",
    "    idx_seqs = [lexicon[word] if word in lexicon else lexicon['<UNK>'] for word in words_list]  \n",
    "    return idx_seqs\n",
    "\n",
    "data['Word_Idxs'] = tokens_to_idxs(words_list, words_lexicon)\n",
    "data['Tag_Idxs'] = tokens_to_idxs(tags_list, tags_lexicon)\n",
    "data[['Sentence #', 'Word', 'Word_Idxs', 'Tag', 'Tag_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-cb1eeb0bb2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence #'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msentence_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence #'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word_Idxs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence #'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtag_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentence #'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Tag_Idxs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 raise TypeError('Could not compare %s type with Series' %\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#condense each sentence to one row\n",
    "sentence_list = []\n",
    "for  i in set(data['Sentence #'].values.tolist()):\n",
    "    sentence_list.append(i)\n",
    "\n",
    "Tokenized_Sentence = []\n",
    "Sentence_Idxs = []\n",
    "Tagged_Sentence = []\n",
    "Tag_Idxs = []\n",
    "\n",
    "for i in sentence_list:\n",
    "    sentence = data.loc[data['Sentence #'] == i, 'Word'].values.tolist()\n",
    "    sentence_idx = data.loc[data['Sentence #'] == i, 'Word_Idxs'].values.tolist()\n",
    "    tag = data.loc[data['Sentence #'] == i, 'Tag'].values.tolist()\n",
    "    tag_idx = data.loc[data['Sentence #'] == i, 'Tag_Idxs'].values.tolist()\n",
    "    \n",
    "    Tokenized_Sentence.append(sentence)\n",
    "    Sentence_Idxs.append( sentence_idx)\n",
    "    Tagged_Sentence.append(tag)\n",
    "    Tag_Idxs.append(tag_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padded\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in Sentence_Idxs])\n",
    "padded_words = pad_idx_seqs(Sentence_Idxs, max_seq_len + 1) \n",
    "padded_tags = pad_idx_seqs(Tag_Idxs,max_seq_len + 1)  \n",
    "\n",
    "print(\"WORDS:\\n\", padded_words)\n",
    "print(\"SHAPE:\", padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", padded_tags)\n",
    "print(\"SHAPE:\", padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data into a train/test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_words, padded_tags, test_size = 0.1, random_state = 1992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Build the model\n",
    "\n",
    "Here we will build a Bidirectional LSTM-CRF model using the `Bidirectional` function from Keras and `CRF` function from Keras-contrib\n",
    "\n",
    "**Documentation and source code:**\n",
    "\n",
    "https://keras.io/layers/wrappers/#bidirectional\n",
    "\n",
    "https://github.com/keras-team/keras-contrib\n",
    "\n",
    "Fit your model with a validation split of 0.1, feel free to use as many epochs as you like. Base your predictions both from the input words **and** the tags from previous words like in the POS example.\n",
    "\n",
    "After building your model, grade your performance on your test set, both by comparing your predicted output to the actual (*at least 3 examples*) and calculate the averaged precision and recall for your tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import numpy\n",
    "from collections import Counter\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "EPOCHS = 5\n",
    "EMBED_DIM = 200\n",
    "BiRNN_UNITS = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the report function\n",
    "def classification_report(y_true, y_pred, labels):\n",
    "    y_true = numpy.asarray(y_true).ravel()\n",
    "    y_pred = numpy.asarray(y_pred).ravel()\n",
    "    corrects = Counter(yt for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
    "    y_true_counts = Counter(y_true)\n",
    "    y_pred_counts = Counter(y_pred)\n",
    "    report = ((lab,  # label\n",
    "               corrects[i] / max(1, y_true_counts[i]),  # recall\n",
    "               corrects[i] / max(1, y_pred_counts[i]),  # precision\n",
    "               y_true_counts[i]  # support\n",
    "               ) for i, lab in enumerate(labels))\n",
    "    report = [(l, r, p, 2 * r * p / max(1e-9, r + p), s) for l, r, p, s in report]\n",
    "\n",
    "    print('{:<15}{:>10}{:>10}{:>10}{:>10}\\n'.format('', 'recall', 'precision', 'f1-score', 'support'))\n",
    "    formatter = '{:<15}{:>10.2f}{:>10.2f}{:>10.2f}{:>10d}'.format\n",
    "    for r in report:\n",
    "        print(formatter(*r))\n",
    "    print('')\n",
    "    report2 = list(zip(*[(r * s, p * s, f1 * s) for l, r, p, f1, s in report]))\n",
    "    N = len(y_true)\n",
    "    print(formatter('avg / total', sum(report2[0]) / N, sum(report2[1]) / N, sum(report2[2]) / N, N) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 200)         7035600   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 200)         240800    \n",
      "_________________________________________________________________\n",
      "crf_2 (CRF)                  (None, None, 18)          3978      \n",
      "=================================================================\n",
      "Total params: 7,280,378\n",
      "Trainable params: 7,280,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c72b9a8cf4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocabulary), EMBED_DIM, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True)))\n",
    "crf = CRF(len(tags_lexicon), sparse_target=True)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "\n",
    "model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "model.fit(x_train[:,1:], y_train[:, 1:, None], epochs=EPOCHS,validation_data=[x_test[:,1:], y_test[:, 1:, None]])\n",
    "\n",
    "y_test_pred = model.predict(x_test).argmax(-1)[x_test > 0]\n",
    "y_test_true = y_test[x_test > 0]\n",
    "\n",
    "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
    "classification_report(y_test_true, y_test_pred, tags_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Pick a dataset\n",
    "\n",
    "Pick a dataset that has short text, similar to the sentences you just tagged. Headlines and tweets are good choices.\n",
    "\n",
    "https://www.kaggle.com/datasets?sortBy=relevance&group=public&search=news&page=1&pageSize=20&size=all&filetype=all&license=all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>win over cena satisfying but defeating underta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Raju Chacha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Status quo will not be disturbed at Ayodhya; s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fissures in Hurriyat over Pak visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>America's unwanted heading for India?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>For bigwigs; it is destination Goa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extra buses to clear tourist traffic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dilute the power of transfers; says Riberio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Focus shifts to teaching of Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IT will become compulsory in schools</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  win over cena satisfying but defeating underta...\n",
       "1                                        Raju Chacha\n",
       "2  Status quo will not be disturbed at Ayodhya; s...\n",
       "3                Fissures in Hurriyat over Pak visit\n",
       "4              America's unwanted heading for India?\n",
       "5                 For bigwigs; it is destination Goa\n",
       "6               Extra buses to clear tourist traffic\n",
       "7        Dilute the power of transfers; says Riberio\n",
       "8                  Focus shifts to teaching of Hindi\n",
       "9               IT will become compulsory in schools"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the dataset I picked is india-news-headlines.csv\n",
    "headline= pd.read_csv(\"india-news-headlines.csv\", encoding=\"latin1\")\n",
    "headline = headline.fillna(method=\"ffill\")\n",
    "headline= headline[:5000]\n",
    "headline = headline.dropna()\n",
    "headline = headline.drop(['publish_date',\n",
    "                'headline_category'], axis=1)\n",
    "headline.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with the dataset: tokenize, word2idx, tag, tag2idx\n",
    "import nltk\n",
    "headline_list = headline.headline_text.values.tolist()\n",
    "tokenize_headlines = []\n",
    "headline_words_list = []\n",
    "for headline in headline_list:\n",
    "    tokenize_headline = nltk.word_tokenize(headline)\n",
    "    tokenize_headlines.append(tokenize_headline)\n",
    "    for word in tokenize_headline:\n",
    "        headline_words_list.append(word)\n",
    "vocabulary_headline = set(headline_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Tag your new data!\n",
    "\n",
    "Create a modification to the **ent_tagger function** that combined words and tags from your original dataset. Now allow the function to also load new text from your new data set, and output the tags predicted from your trained model alongside the text. Make your function load five random texts from your data and output the tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define ent_tagger function using words list\n",
    "\n",
    "ent_tagger_list = []\n",
    "headline_tags_list = []\n",
    "def ent_tagger(tokenize_headlines_sequences):\n",
    "    for headline in tokenize_headlines_sequences:\n",
    "        headline_tag_list = []\n",
    "        for word in headline:\n",
    "            if word in vocabulary:\n",
    "                tag = data.loc[data['Word'] == word, 'Tag'].values.tolist()[0]\n",
    "            else:\n",
    "                tag = '<UNK>'\n",
    "            tagger = (word,tag)\n",
    "            headline_tag_list.append(tag)\n",
    "            ent_tagger_list.append(tagger)\n",
    "        headline_tags_list.append(headline_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "ent_tagger(tokenize_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      "LEXICON SAMPLE (9123 total items):\n",
      "{'win': 2, 'over': 3, 'cena': 4, 'satisfying': 5, 'but': 6, 'defeating': 7, 'undertaker': 8, 'bigger': 9, 'roman': 10, 'reigns': 11, 'Raju': 12, 'Chacha': 13, 'Status': 14, 'quo': 15, 'will': 16, 'not': 17, 'be': 18, 'disturbed': 19, 'at': 20, 'Ayodhya': 21}\n",
      "TAGS:\n",
      "LEXICON SAMPLE (1 total items):\n",
      "{'<UNK>': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"WORDS:\")\n",
    "words_lexicon = make_lexicon(headline_words_list)\n",
    "\n",
    "print(\"TAGS:\")\n",
    "tags_lexicon = make_lexicon(headline_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEXICON LOOKUP SAMPLE:\n",
      "{2: 'O', 3: 'B-geo', 4: 'B-gpe', 5: 'B-per', 6: 'I-geo', 7: 'B-org', 8: 'I-org', 9: 'B-tim', 10: 'B-art', 11: 'I-art', 12: 'I-per', 13: 'I-gpe', 14: 'I-tim', 15: 'B-nat', 16: 'B-eve', 17: 'I-eve', 18: 'I-nat', 1: '<UNK>'}\n"
     ]
    }
   ],
   "source": [
    "#get tags lookup\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "headline_idx = tokens_to_idxs(tokenize_headlines, words_lexicon)\n",
    "headline_tag_idx = tokens_to_idxs(headline_tags_list, tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS:\n",
      " [[    0     0     0 ...,  8967     1     1]\n",
      " [    0     0     0 ...,     0     1     1]\n",
      " [    0     0     0 ...,  3235   570 32540]\n",
      " ..., \n",
      " [    0     0     0 ...,     9  4286   171]\n",
      " [    0     0     0 ...,    55   398     1]\n",
      " [    0     0     0 ...,   431   314     1]]\n",
      "SHAPE: (5000, 99) \n",
      "\n",
      "TAGS:\n",
      " [[ 0  0  0 ...,  2  1  1]\n",
      " [ 0  0  0 ...,  0  1  1]\n",
      " [ 0  0  0 ...,  2  2 12]\n",
      " ..., \n",
      " [ 0  0  0 ...,  2  2  5]\n",
      " [ 0  0  0 ...,  2  2  1]\n",
      " [ 0  0  0 ...,  2  2  1]]\n",
      "SHAPE: (5000, 99) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#padded the new dataset\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in headline_idx])\n",
    "padded_headlines = pad_idx_seqs(headline_idx, max_seq_len + 1) \n",
    "padded_headline_tags = pad_idx_seqs(headline_tag_idx,max_seq_len + 1)  \n",
    "\n",
    "print(\"WORDS:\\n\", padded_headlines)\n",
    "print(\"SHAPE:\", padded_headlines.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\\n\", padded_headline_tags)\n",
    "print(\"SHAPE:\", padded_headline_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Result of BiLSTM-CRF ----\n",
      "\n",
      "                   recall precision  f1-score   support\n",
      "\n",
      "O                    0.00      0.00      0.00         0\n",
      "B-geo                0.00      0.21      0.00      7322\n",
      "B-gpe                0.00      0.00      0.00     21421\n",
      "B-per                0.00      0.00      0.00       610\n",
      "I-geo                0.00      0.00      0.00       176\n",
      "B-org                0.98      0.01      0.02       288\n",
      "I-org                0.00      0.00      0.00       126\n",
      "B-tim                0.00      0.00      0.00       514\n",
      "B-art                0.00      0.00      0.00       606\n",
      "I-art                0.00      0.00      0.00       118\n",
      "I-per                0.00      0.00      0.00        28\n",
      "I-gpe                0.00      0.00      0.00        88\n",
      "I-tim                0.00      0.00      0.00       389\n",
      "B-nat                0.00      0.00      0.00        23\n",
      "B-eve                0.00      0.00      0.00       125\n",
      "I-eve                0.00      0.00      0.00        14\n",
      "I-nat                0.00      0.00      0.00         9\n",
      "<UNK>                0.03      0.00      0.00        37\n",
      "\n",
      "avg / total          0.01      0.05      0.00     31894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict the tags\n",
    "y_test_pred = model.predict(padded_headlines).argmax(-1)[padded_headlines > 0]\n",
    "y_test_true = padded_headline_tags[padded_headlines > 0]\n",
    "\n",
    "print('\\n---- Result of BiLSTM-CRF ----\\n')\n",
    "classification_report(y_test_true, y_test_pred, tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label:    ['O', 'O', '<UNK>', 'O', 'O', 'O', '<UNK>', 'O', '<UNK>', '<UNK>']\n",
      "Predict label: ['B-per', 'B-per', 'B-per', 'B-per', 'B-per', 'B-per', 'B-per', 'B-per', 'B-per', 'B-per']\n"
     ]
    }
   ],
   "source": [
    "#print out true tags and predict tags to have a direct view\n",
    "y_test_true_tag = []\n",
    "for i in y_test_true[:10]:\n",
    "    tag = tags_lexicon_lookup[i]\n",
    "    y_test_true_tag.append(tag)\n",
    "    \n",
    "y_test_pred_tag = []\n",
    "for i in y_test_pred[:10]:\n",
    "    tag = tags_lexicon_lookup[i]\n",
    "    y_test_pred_tag.append(tag)  \n",
    "\n",
    "print('True label:   ',y_test_true_tag)\n",
    "print('Predict label:',y_test_pred_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
